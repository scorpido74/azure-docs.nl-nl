---
title: Gegevens verplaatsen in ML-pijp lijnen
titleSuffix: Azure Machine Learning
description: Meer informatie over invoer & uitvoer van gegevens in Azure Machine Learning pijp lijnen.
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.author: laobri
author: lobrien
ms.date: 08/20/2020
ms.topic: conceptual
ms.custom: how-to, contperfq4, devx-track-python
ms.openlocfilehash: 1b6b5af2e6533c13165ae8253813a52b2c7ad261
ms.sourcegitcommit: afa1411c3fb2084cccc4262860aab4f0b5c994ef
ms.translationtype: MT
ms.contentlocale: nl-NL
ms.lasthandoff: 08/23/2020
ms.locfileid: "88756959"
---
# <a name="moving-data-into-and-between-ml-pipeline-steps-python"></a>Gegevens verplaatsen naar en tussen ML-pijplijnstappen (Python)

[!INCLUDE [applies-to-skus](../../includes/aml-applies-to-basic-enterprise-sku.md)]

Dit artikel bevat code voor het importeren, transformeren en verplaatsen van gegevens tussen stappen in een Azure Machine Learning-pijp lijn. Zie [toegang tot gegevens in azure Storage-services](how-to-access-data.md)voor een overzicht van de werking van gegevens in azure machine learning. Zie [Wat zijn Azure machine learning pijp lijnen?](concept-ml-pipelines.md)voor de voor delen en de structuur van Azure machine learning pijp lijnen.

In dit artikel wordt uitgelegd hoe u:

- `Dataset`Objecten gebruiken voor bestaande gegevens
- Toegang tot gegevens in uw stappen
- `Dataset`Gegevens splitsen in subsets, zoals training en validatie subsets
- `OutputFileDatasetConfig`Objecten maken voor het overdragen van gegevens naar de volgende pijplijn stap
- `OutputFileDatasetConfig`Objecten gebruiken als invoer voor pijplijn stappen
- Nieuwe `Dataset` objecten maken die `OutputFileDatasetConfig` u wilt behouden

> [!NOTE]
>De `OutputFileDatasetConfig` `OutputTabularDatasetConfig` klassen en zijn experimentele preview-functies en kunnen op elk gewenst moment worden gewijzigd.
>
>Voor meer informatie raadpleegt u https://aka.ms/azuremlexperimental.

## <a name="prerequisites"></a>Vereisten

U hebt het volgende nodig:

- Een Azure-abonnement. Als u geen Azure-abonnement hebt, maakt u een gratis account voordat u begint. Probeer de [gratis of betaalde versie van Azure machine learning](https://aka.ms/AMLFree).

- De [Azure machine learning SDK voor python](https://docs.microsoft.com/python/api/overview/azure/ml/intro?view=azure-ml-py)of toegang tot [Azure machine learning Studio](https://ml.azure.com/).

- Een Azure Machine Learning-werkruimte.
  
  [Maak een Azure machine learning-werk ruimte](how-to-manage-workspace.md) of gebruik een bestaand item via de PYTHON-SDK. Importeer de `Workspace` `Datastore` -en-klasse en laad uw abonnements gegevens uit het bestand `config.json` met behulp van de functie `from_config()` . Deze functie zoekt standaard naar het JSON-bestand in de huidige map, maar u kunt ook een para meter Path opgeven om naar het bestand te verwijzen met `from_config(path="your/file/path")` .

   ```python
   import azureml.core
   from azureml.core import Workspace, Datastore
        
   ws = Workspace.from_config()
   ```

- Een aantal bestaande gegevens. In dit artikel wordt een kort overzicht gegeven van het gebruik van een [Azure Blob-container](https://docs.microsoft.com/azure/storage/blobs/storage-blobs-overview).

- Optioneel: een bestaande machine learning pijp lijn, zoals de pijplijn die wordt beschreven in [machine learning pijp lijnen maken en uitvoeren met Azure machine learning SDK](how-to-create-your-first-pipeline.md).

## <a name="use-dataset-objects-for-pre-existing-data"></a>`Dataset`Objecten gebruiken voor bestaande gegevens 

De voorkeurs manier om gegevens op te nemen in een pijp lijn is het gebruik van een object [DataSet](https://docs.microsoft.com/python/api/azureml-core/azureml.core.dataset%28class%29?view=azure-ml-py) . `Dataset` objecten vertegenwoordigen permanente gegevens die beschikbaar zijn in een werk ruimte.

Er zijn veel manieren om objecten te maken en te registreren `Dataset` . Tabellaire gegevens sets zijn voor een scheidings teken dat beschikbaar is in een of meer bestanden. Bestand gegevens sets zijn voor binaire gegevens (zoals installatie kopieÃ«n) of voor gegevens die u gaat parseren. De eenvoudigste manier om objecten te maken `Dataset` , is het gebruik van bestaande blobs in werkruimte opslag of open bare url's:

```python
datastore = Datastore.get(workspace, 'training_data')
iris_dataset = Dataset.Tabular.from_delimited_files(DataPath(datastore, 'iris.csv'))

cats_dogs_dataset = Dataset.File.from_files(
    paths='https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip',
    archive_options=ArchiveOptions(archive_type=ArchiveType.ZIP, entry_glob='**/*.jpg')
)
```

Zie [Azure machine learning gegevens sets maken](how-to-create-register-datasets.md)voor meer opties voor het maken van gegevens sets met verschillende opties en uit verschillende bronnen, het registreren van de gegevens in de gebruikers interface van de Azure machine learning. 

### <a name="pass-datasets-to-your-script"></a>Gegevens sets door geven aan het script

Als u het pad van de gegevensset wilt door geven aan uw script, gebruikt u de `Dataset` methode van het object `as_named_input()` . U kunt het resulterende object door geven `DatasetConsumptionConfig` aan uw script als een argument of, met behulp van het `inputs` argument voor uw pijplijn script, u de gegevensset kunt ophalen met `Run.get_context().input_datasets[]` .

Zodra u een benoemde invoer hebt gemaakt, kunt u de toegangs modus kiezen: `as_mount()` of `as_download()` . Als uw script alle bestanden in uw gegevensset verwerkt en de schijf op de reken resource groot genoeg is voor de gegevensset, is de Download toegangs modus de beste keuze. De Download toegangs modus voor komt dat de overhead van het streamen van gegevens tijdens runtime wordt voor komen. Als uw script toegang krijgt tot een subset van de gegevensset of als deze te groot is voor uw compute, moet u de toegangs modus voor koppelen gebruiken. Lees [koppelen versus downloaden](https://docs.microsoft.com/azure/machine-learning/how-to-train-with-datasets#mount-vs-download) voor meer informatie.

Een gegevensset door geven aan de pijplijn stap:

1. Gebruik `TabularDataset.as_named_inputs()` of `FileDataset.as_named_input()` (geen ' aan het einde) om een object te maken `DatasetConsumptionConfig`
1. `as_mount()` `as_download()` De toegangs modus gebruiken of instellen
1. Geef de gegevens sets door aan de pijplijn stappen met behulp van het `arguments` of het `inputs` argument

Het volgende code fragment toont het algemene patroon van het combi neren van deze stappen in de `PythonScriptStep` constructor: 

```python

train_step = PythonScriptStep(
    name="train_data",
    script_name="train.py",
    compute_target=cluster,
    inputs=[iris_dataset.as_named_inputs('iris').as_mount()]
)
```

U kunt ook methoden als gebruiken `random_split()` `take_sample()` om meerdere invoer te maken of de hoeveelheid gegevens te verminderen die worden door gegeven aan de pijplijn stap:

```python
seed = 42 # PRNG seed
smaller_dataset = iris_dataset.take_sample(0.1, seed=seed) # 10%
train, test = smaller_dataset.random_split(percentage=0.8, seed=seed)

train_step = PythonScriptStep(
    name="train_data",
    script_name="train.py",
    compute_target=cluster,
    inputs=[train.as_named_inputs('train').as_download(), test.as_named_inputs('test').as_download()]
)
```

### <a name="access-datasets-within-your-script"></a>Toegang tot gegevens sets in uw script

Benoemde invoer van het script voor de pijplijn stap is beschikbaar als een woorden lijst binnen het `Run` object. Haal het actieve `Run` object op met behulp `Run.get_context()` van en haal vervolgens de woorden lijst met de naam invoer op met behulp van `input_datasets` . Als u het object hebt door gegeven `DatasetConsumptionConfig` met behulp `arguments` van het argument in plaats van het `inputs` argument, opent u de gegevens met behulp van `ArgParser` code. Beide technieken worden in het volgende code fragment getoond.

```python
# In pipeline definition script:
# Code for demonstration only: It would be very confusing to split datasets between `arguments` and `inputs`
train_step = PythonScriptStep(
    name="train_data",
    script_name="train.py",
    compute_target=cluster,
    arguments=['--training-folder', train.as_named_inputs('train').as_download()]
    inputs=[test.as_named_inputs('test').as_download()]
)

# In pipeline script
parser = argparse.ArgumentParser()
parser.add_argument('--training-folder', type=str, dest='train_folder', help='training data folder mounting point')
args = parser.parse_args()
training_data_folder = args.train_folder

testing_data_folder = Run.get_context().input_datasets['test']
```

De door gegeven waarde is het pad naar de bestand (en) van de gegevensset.

Het is ook mogelijk om rechtstreeks toegang te krijgen tot een geregistreerd `Dataset` . Omdat geregistreerde gegevens sets permanent zijn en worden gedeeld in een werk ruimte, kunt u ze direct ophalen:

```python
run = Run.get_context()
ws = run.experiment.workspace
ds = Dataset.get_by_name(workspace=ws, name='mnist_opendataset')
```

## <a name="use-outputfiledatasetconfig-for-intermediate-data"></a>Gebruiken `OutputFileDatasetConfig` voor tussenliggende gegevens

Hoewel `Dataset` objecten alleen persistente gegevens vertegenwoordigen, [`OutputFileDatasetConfig`](https://docs.microsoft.com/python/api/azureml-core/azureml.data.outputfiledatasetconfig?view=azure-ml-py) kunnen object (en) worden gebruikt voor tijdelijke gegevens uitvoer van pijplijn stappen **en** permanente uitvoer gegevens. 

 `OutputFileDatasetConfig` het standaard gedrag van het object is om te schrijven naar de standaard gegevens opslag van de werk ruimte. Geef uw `OutputFileDatasetConfig` objecten door aan uw `PythonScriptStep` met de `arguments` para meter.

```python
from azureml.data import OutputFileDatasetConfig
dataprep_output = OutputFileDatasetConfig()
input_dataset = Dataset.get_by_name(workspace, 'raw_data')

dataprep_step = PythonScriptStep(
    name="prep_data",
    script_name="dataprep.py",
    compute_target=cluster,
    arguments=[input_dataset.as_named_input('raw_data').as_mount(), dataprep_output]
    )
```

U kunt ervoor kiezen om de inhoud van uw `OutputFileDatasetConfig` object te uploaden aan het einde van een run. In dat geval gebruikt u de `as_upload()` functie samen met uw `OutputFileDatasetConfig` object en geeft u op of bestaande bestanden in de bestemming moeten worden overschreven. 

```python
#get blob datastore already registered with the workspace
blob_store= ws.datastores['my_blob_store']
OutputFileDatasetConfig(name="clean_data", destination=blob_store).as_upload(overwrite=False)
```

### <a name="use-outputfiledatasetconfig-as-outputs-of-a-training-step"></a>Gebruiken `OutputFileDatasetConfig` als uitvoer van een trainings stap

Binnen de pijp lijn `PythonScriptStep` kunt u de beschik bare uitvoer paden ophalen met behulp van de argumenten van het programma. Als deze stap het eerst is en de uitvoer gegevens initialiseert, moet u de Directory maken op het opgegeven pad. Vervolgens kunt u de bestanden die u wilt opnemen in de opslaan `OutputFileDatasetConfig` .

```python
parser = argparse.ArgumentParser()
parser.add_argument('--output_path', dest='output_path', required=True)
args = parser.parse_args()

# Make directory for file
os.makedirs(os.path.dirname(args.output_path), exist_ok=True)
with open(args.output_path, 'w') as f:
    f.write("Step 1's output")
```

### <a name="read-outputfiledatasetconfig-as-inputs-to-non-initial-steps"></a>Lezen `OutputFileDatasetConfig` als invoer van niet-eerste stappen

Nadat de eerste pijplijn stap enkele gegevens naar het `OutputFileDatasetConfig` pad heeft geschreven en deze een uitvoer van die eerste stap wordt, kan deze worden gebruikt als invoer voor een latere stap. 

In de volgende code, 

* `step1_output_data` geeft aan dat de uitvoer van de PythonScriptStep, `step1` wordt geschreven naar het gegevens archief van ADLS gen 2 `my_adlsgen2` in de modus voor het uploaden van toegang. Meer informatie over het [instellen van rolmachtigingen](how-to-access-data.md#azure-data-lake-storage-generation-2) voor het terugschrijven van gegevens naar ADLS gen 2-gegevens opslag. 

* Nadat `step1` de uitvoering is voltooid en de uitvoer is geschreven naar de bestemming aangegeven door `step1_output_data` , is step2 klaar om te worden gebruikt `step1_output_data` als invoer. 

```python
# get adls gen 2 datastore already registered with the workspace
datastore = workspace.datastores['my_adlsgen2']
step1_output_data = OutputFileDatasetConfig(name="processed_data", destination=datastore).as_upload()

step1 = PythonScriptStep(
    name="generate_data",
    script_name="step1.py",
    runconfig = aml_run_config,
    arguments = ["--output_path", step1_output_data]
)

step2 = PythonScriptStep(
    name="read_pipeline_data",
    script_name="step2.py",
    compute_target=compute,
    runconfig = aml_run_config,
    arguments = ["--pd", step1_output_data.as_input]

)

pipeline = Pipeline(workspace=ws, steps=[step1, step2])
```

## <a name="register-outputfiledatasetconfig-objects-for-reuse"></a>`OutputFileDatasetConfig`Objecten registreren voor hergebruik

Als u `OutputFileDatasetConfig` langer dan de duur van uw experiment beschikbaar wilt maken, registreert u deze voor uw werk ruimte om te delen en opnieuw te gebruiken in experimenten.

```python
step1_output_ds = step1_output_data.register_on_complete(name='processed_data', 
                                                         description = 'files from step1`)
```

## <a name="next-steps"></a>Volgende stappen

* [Een Azure machine learning-gegevensset maken](how-to-create-register-datasets.md)
* [machine learning-pijp lijnen maken en uitvoeren met Azure Machine Learning SDK](how-to-create-your-first-pipeline.md)
