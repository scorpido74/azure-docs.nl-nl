---
title: Apache Kafka verbinding met Azure Event Hubs (preview) integreren met Debezium voor Change Data Capture
description: In dit artikel vindt u informatie over het gebruik van Apache Spark met Azure Event Hubs voor Kafka.
ms.topic: how-to
author: abhirockzz
ms.author: abhishgu
ms.date: 08/11/2020
ms.openlocfilehash: a11ec882a50d051a34758562ac84dcef5b799f5f
ms.sourcegitcommit: 1aef4235aec3fd326ded18df7fdb750883809ae8
ms.translationtype: MT
ms.contentlocale: nl-NL
ms.lasthandoff: 08/12/2020
ms.locfileid: "88136889"
---
# <a name="integrate-apache-kafka-connect-support-on-azure-event-hubs-preview-with-debezium-for-change-data-capture"></a>Apache Kafka Connect-ondersteuning op Azure Event Hubs (preview) integreren met Debezium voor Change Data Capture

**Change Data Capture (CDC)** is een techniek die wordt gebruikt voor het bijhouden van wijzigingen op rijniveau in database tabellen in reactie op Create-, update-en delete-bewerkingen. [Debezium](https://debezium.io/) is een gedistribueerd platform dat is gebaseerd op wijzigingen in de functies voor het vastleggen van gegevens die beschikbaar zijn in verschillende data bases (bijvoorbeeld [logische decodering in postgresql](https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html)). Het bevat een set [Kafka Connect-connectors](https://debezium.io/documentation/reference/1.2/connectors/index.html) die op wijzigingen in rijen op rijniveau tikken in database tabellen (en) en deze converteren naar gebeurtenis stromen die vervolgens naar [Apache Kafka](https://kafka.apache.org/)worden verzonden.

In deze zelf studie leert u hoe u een op change data capture gebaseerd systeem op Azure kunt instellen met behulp van [azure Event hubs](https://docs.microsoft.com/azure/event-hubs/event-hubs-about?WT.mc_id=devto-blog-abhishgu) (voor Kafka), [Azure DB voor postgresql](../postgresql/overview.md) en Debezium. De [Debezium postgresql-connector](https://debezium.io/documentation/reference/1.2/connectors/postgresql.html) wordt gebruikt voor het streamen van database wijzigingen van postgresql naar Kafka-onderwerpen in azure Event hubs

In deze zelfstudie voert u de volgende stappen uit:

> [!div class="checklist"]
> * Een Event Hubs-naamruimte maken
> * Azure Database for PostgreSQL instellen en configureren
> * Kafka Connect met Debezium PostgreSQL-connector configureren en uitvoeren
> * change data capture testen
> * Beschrijving Change Data-gebeurtenissen gebruiken met een `FileStreamSink` connector

## <a name="pre-requisites"></a>Vereisten
Als u deze procedure wilt volt ooien, hebt u het volgende nodig:

- Azure-abonnement. Als u nog geen account hebt, kunt u [een gratis account maken](https://azure.microsoft.com/free/).
- Linux/Mac OS
- Kafka-release (versie 1.1.1, Scala-versie 2.11), beschikbaar op [kafka.apache.org](https://kafka.apache.org/downloads#1.1.1)
- Lees het inleidende artikel [Event Hubs voor Apache Kafka](./event-hubs-for-kafka-ecosystem-overview.md) door

## <a name="create-an-event-hubs-namespace"></a>Een Event Hubs-naamruimte maken
Er is een Event Hubs-naamruimte vereist om gegevens te verzenden naar en te ontvangen van Event Hubs-services. Zie [een event hub maken](event-hubs-create.md) voor instructies voor het maken van een naam ruimte en een event hub. Haal de Event Hubs-verbindingsreeks en de Fully Qualified Domain Name (FQDN) op voor later gebruik. Zie [Get an Event Hubs connection string](event-hubs-get-connection-string.md). (Een Event Hubs-verbindingsreeks ophalen). 

## <a name="setup-and-configure-azure-database-for-postgresql"></a>Azure Database for PostgreSQL instellen en configureren
[Azure database for PostgreSQL](../postgresql/overview.md) is een relationele database service op basis van de Community-versie van open-source PostgreSQL data base engine en is beschikbaar in twee implementatie opties: één server en grootschalige (Citus). [Volg deze instructies](../postgresql/quickstart-create-server-database-portal.md) om een Azure database for postgresql server te maken met behulp van de Azure Portal. 

## <a name="setup-and-run-kafka-connect"></a>Kafka-verbinding instellen en uitvoeren
In deze sectie komen de volgende onderwerpen aan bod:

- Installatie van Debezium-connector
- Kafka Connect configureren voor Event Hubs
- Kafka Connect-cluster met Debezium-connector starten

### <a name="download-and-setup-debezium-connector"></a>Debezium-connector downloaden en instellen
Volg de laatste instructies in de [Debezium-documentatie](https://debezium.io/documentation/reference/1.2/connectors/postgresql.html#postgresql-deploying-a-connector) om de connector te downloaden en in te stellen.

- Down load het invoeg toepassings archief van de connector. Als u bijvoorbeeld `1.2.0` de versie van de connector wilt downloaden, gebruikt u deze koppeling:https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/1.2.0.Final/debezium-connector-postgres-1.2.0.Final-plugin.tar.gz
- Pak de JAR-bestanden uit en kopieer deze naar de [Kafka Connect-invoeg toepassing. pad](https://kafka.apache.org/documentation/#connectconfigs).


### <a name="configure-kafka-connect-for-event-hubs"></a>Kafka Connect configureren voor Event Hubs
Er is een minimale herconfiguratie vereist als u doorvoer van Kafka Connect wilt omleiden van Kafka naar Event Hubs.  In het volgende `connect-distributed.properties`-voorbeeld wordt getoond hoe u Connect kunt configureren om op Event Hubs het Kafka-eindpunt te verifiëren en hoe ermee te communiceren:

```properties
bootstrap.servers={YOUR.EVENTHUBS.FQDN}:9093 # e.g. namespace.servicebus.windows.net:9093
group.id=connect-cluster-group

# connect internal topic names, auto-created if not exists
config.storage.topic=connect-cluster-configs
offset.storage.topic=connect-cluster-offsets
status.storage.topic=connect-cluster-status

# internal topic replication factors - auto 3x replication in Azure Storage
config.storage.replication.factor=1
offset.storage.replication.factor=1
status.storage.replication.factor=1

rest.advertised.host.name=connect
offset.flush.interval.ms=10000

key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
internal.key.converter=org.apache.kafka.connect.json.JsonConverter
internal.value.converter=org.apache.kafka.connect.json.JsonConverter

internal.key.converter.schemas.enable=false
internal.value.converter.schemas.enable=false

# required EH Kafka security settings
security.protocol=SASL_SSL
sasl.mechanism=PLAIN
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="$ConnectionString" password="{YOUR.EVENTHUBS.CONNECTION.STRING}";

producer.security.protocol=SASL_SSL
producer.sasl.mechanism=PLAIN
producer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="$ConnectionString" password="{YOUR.EVENTHUBS.CONNECTION.STRING}";

consumer.security.protocol=SASL_SSL
consumer.sasl.mechanism=PLAIN
consumer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="$ConnectionString" password="{YOUR.EVENTHUBS.CONNECTION.STRING}";

plugin.path={KAFKA.DIRECTORY}/libs # path to the libs directory within the Kafka release
```

### <a name="run-kafka-connect"></a>Kafka Connect uitvoeren
In deze stap wordt een Kafka Connect-werkrol lokaal wordt gestart in gedistribueerde modus, waarbij Event Hubs wordt gebruikt om de clusterstatus te handhaven.

1. Sla het bovenstaande `connect-distributed.properties`-bestand lokaal op.  Vervang alle waarden tussen accolades.
2. Ga naar de locatie van de Kafka-release op uw computer.
3. Voer uit `./bin/connect-distributed.sh /PATH/TO/connect-distributed.properties` en wacht totdat het cluster wordt gestart.

> [!NOTE]
> Kafka Connect maakt gebruik van de Kafka AdminClient-API om automatisch onderwerpen te maken met aanbevolen configuraties, inclusief compressie. Een snelle controle van de naamruimte in de Azure-portal laat zien dat de interne onderwerpen van de Connect-werkrol automatisch zijn gemaakt.
>
> Kafka Connect interne onderwerpen **moeten gebruikmaken van compressie**.  Het Event Hubs-team is niet verantwoordelijk voor het oplossen van onjuiste configuraties als interne Connect-onderwerpen onjuist zijn geconfigureerd.

### <a name="configure-and-start-the-debezium-postgresql-source-connector"></a>De Debezium PostgreSQL-bron connector configureren en starten

Maak een configuratie bestand ( `pg-source-connector.json` ) voor de postgresql-bron connector, vervang de waarden door uw Azure postgresql-instantie.

```json
{
    "name": "todo-connector",
    "config": {
        "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
        "database.hostname": "<replace with Azure PostgreSQL instance name>.postgres.database.azure.com",
        "database.port": "5432",
        "database.user": "<replace with database user name>",
        "database.password": "<replace with database password>",
        "database.dbname": "postgres",
        "database.server.name": "my-server",
        "plugin.name": "wal2json",
        "table.whitelist": "public.todos"
    }
}
```

> [!TIP]
> `database.server.name`het kenmerk is een logische naam die een naam ruimte identificeert en biedt voor de specifieke PostgreSQL-database server of het cluster dat wordt bewaakt. Raadpleeg de [Debezium-documentatie](https://debezium.io/documentation/reference/1.2/connectors/postgresql.html#postgresql-property-database-server-name) voor gedetailleerde informatie.

Als u een exemplaar van de connector wilt maken, gebruikt u het Kafka Connect REST API-eind punt:

```bash
curl -X POST -H "Content-Type: application/json" --data @pg-source-connector.json http://localhost:8083/connectors
```

De status van de connector controleren:

```bash
curl -s http://localhost:8083/connectors/todo-connector/status
```

## <a name="test-change-data-capture"></a>change data capture testen
Als u change data capture in actie wilt zien, moet u records maken/bijwerken/verwijderen in de Azure PostgreSQL-data base.

Maak eerst verbinding met uw Azure PostgreSQL-data base (in het onderstaande voor beeld wordt [psql](https://www.postgresql.org/docs/12/app-psql.html)gebruikt)

```bash
psql -h <POSTGRES_INSTANCE_NAME>.postgres.database.azure.com -p 5432 -U <POSTGRES_USER_NAME> -W -d <POSTGRES_DB_NAME> --set=sslmode=require

e.g. 

psql -h my-postgres.postgres.database.azure.com -p 5432 -U testuser@my-postgres -W -d postgres --set=sslmode=require
```

**Een tabel maken en records invoegen**

```sql
CREATE TABLE todos (id SERIAL, description VARCHAR(50), todo_status VARCHAR(10), PRIMARY KEY(id));

INSERT INTO todos (description, todo_status) VALUES ('setup postgresql on azure', 'complete');
INSERT INTO todos (description, todo_status) VALUES ('setup kafka connect', 'complete');
INSERT INTO todos (description, todo_status) VALUES ('configure and install connector', 'in-progress');
INSERT INTO todos (description, todo_status) VALUES ('start connector', 'pending');
```

De connector moet nu actie ondernemen en wijzigings gegevens gebeurtenissen verzenden naar een Event Hubs onderwerp met de volgende nb, e `my-server.public.todos` , ervan uitgaande dat u de `my-server` waarde voor hebt `database.server.name` en `public.todos` de tabel waarvan u de wijzigingen bijhoudt (per `table.whitelist` configuratie)

**Event Hubs onderwerp controleren**

We introspect de inhoud van het onderwerp om te controleren of alles werkt zoals verwacht. In het onderstaande voor beeld wordt gebruikt [`kafkacat`](https://github.com/Azure/azure-event-hubs-for-kafka/tree/master/quickstart/kafkacat) , maar u kunt ook [een consument maken met behulp van een van de opties die hier worden vermeld](apache-kafka-developer-guide.md)

Maak een bestand `kafkacat.conf` met de naam met de volgende inhoud:

```
metadata.broker.list=<enter event hubs namespace>.servicebus.windows.net:9093
security.protocol=SASL_SSL
sasl.mechanisms=PLAIN
sasl.username=$ConnectionString
sasl.password=<enter event hubs connection string>
```

> [!NOTE]
> Update `metadata.broker.list` en `sasl.password` kenmerken in de `kafkacat.conf` vorm van gegevens per Event hubs. 

Start een consument in een andere terminal:

```bash
export KAFKACAT_CONFIG=kafkacat.conf
export BROKER=<enter event hubs namespace>.servicebus.windows.net:9093
export TOPIC=my-server.public.todos

kafkacat -b $BROKER -t $TOPIC -o beginning
```

Als antwoord op de rijen die u zojuist hebt toegevoegd aan de tabel, ziet u de JSON-nettoladingen die de wijzigings gegevens gebeurtenissen vertegenwoordigen die zijn gegenereerd in PostgreSQL `todos` . Hier volgt een code fragment van de payload:


```json
{
    "schema": {...},
    "payload": {
        "before": null,
        "after": {
            "id": 1,
            "description": "setup postgresql on azure",
            "todo_status": "complete"
        },
        "source": {
            "version": "1.2.0.Final",
            "connector": "postgresql",
            "name": "fullfillment",
            "ts_ms": 1593018069944,
            "snapshot": "last",
            "db": "postgres",
            "schema": "public",
            "table": "todos",
            "txId": 602,
            "lsn": 184579736,
            "xmin": null
        },
        "op": "c",
        "ts_ms": 1593018069947,
        "transaction": null
    }
```

De gebeurtenis bestaat uit de `payload` samen met de `schema` (weglatende voor boog). In `payload` het gedeelte ziet u hoe de bewerking maken ( `"op": "c"` ) wordt weer gegeven `"before": null` . Dit houdt in dat het een nieuwe `INSERT` rij is, met `after` waarden voor de kolommen in de rij, `source` de meta gegevens van het postgresql-exemplaar van waaruit deze gebeurtenis is opgehaald, enzovoort.

U kunt dezelfde bewerking ook uitvoeren met Update-of Delete-bewerkingen en de introspect wijzigen. Als u bijvoorbeeld de taak status wilt bijwerken voor `configure and install connector` (ervan uitgaande dat deze `id` is `3` ):

```sql
UPDATE todos SET todo_status = 'complete' WHERE id = 3;
```

## <a name="optional-install-filestreamsink-connector"></a>Beschrijving FileStreamSink-connector installeren
Nu alle `todos` wijzigingen in de tabel in Event hubs onderwerp worden vastgelegd, zullen we de FileStreamSink-connector (die standaard beschikbaar is in Kafka Connect) gebruiken om deze gebeurtenissen te verbruiken.

Een configuratie bestand ( `file-sink-connector.json` ) voor de connector maken-het kenmerk vervangen door `file` uw bestands systeem

```json
{
    "name": "cdc-file-sink",
    "config": {
        "connector.class": "org.apache.kafka.connect.file.FileStreamSinkConnector",
        "tasks.max": "1",
        "topics": "my-server.public.todos",
        "file": "<enter full path to file e.g. /Users/foo/todos-cdc.txt>"
    }
}
```

De connector maken en de status ervan controleren:

```bash
curl -X POST -H "Content-Type: application/json" --data @file-sink-connector.json http://localhost:8083/connectors

curl http://localhost:8083/connectors/cdc-file-sink/status
```

Data base records invoegen/bijwerken/verwijderen en de records bewaken in het geconfigureerde uitvoer Sink-bestand:

```bash
tail -f /Users/foo/todos-cdc.txt
```


## <a name="cleanup"></a>Opschonen
Kafka Connect maakt Event Hub-onderwerpen om configuraties, verschuivingen en status op te slaan die ook behouden blijven nadat het Connect-cluster is uitgeschakeld. Tenzij persistentie gewenst is, wordt het aanbevolen deze onderwerpen te verwijderen. Mogelijk wilt u ook de `my-server.public.todos` Event hub verwijderen die tijdens de uitvoering van deze procedure is gemaakt.

## <a name="next-steps"></a>Volgende stappen

Raadpleeg de volgende artikelen voor meer informatie over Event Hubs voor Kafka:  

- [Een Kafka-broker spiegelen in een Event Hub](event-hubs-kafka-mirror-maker-tutorial.md)
- [Apache Spark aan een Event Hub koppelen](event-hubs-kafka-spark-tutorial.md)
- [Apache Flink aan een Event Hub koppelen](event-hubs-kafka-flink-tutorial.md)
- [Explore samples on our GitHub](https://github.com/Azure/azure-event-hubs-for-kafka) (Voorbeelden bekijken op GitHub)
- [Akka-streams verbinden met een Event Hub](event-hubs-kafka-akka-streams-tutorial.md)
- [Apache Kafka ontwikkelaars handleiding voor Azure Event Hubs](apache-kafka-developer-guide.md)
